{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the finished functions related to Project_607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(dataset, tfidf=False, vec_size=5):\n",
    "    '''\n",
    "    setup prepares the NLP for the movie dataset by creating a cosine similarity matrix\n",
    "    and a movie index for reference\n",
    "    *****\n",
    "    Paramters\n",
    "    ---------\n",
    "    dataset : dataframe,\n",
    "        Movie data in a pandas dataframe\n",
    "    tfidf : boolean,\n",
    "        Switch that enables the use of TFIDF vectorization. Default to False\n",
    "    vec_size : int,\n",
    "        Size of the output Doc2Vec vectors\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : dataframe,\n",
    "        modified dataframe, including vector column\n",
    "    cossim : np.array,\n",
    "        cosine similarity matrix for the entire dataset\n",
    "    movie_index : dictionary,\n",
    "        movie index to correlate the movie dataframe with the cosine similarity matrix\n",
    "    duplicates : list,\n",
    "        list of duplicate movies\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if tfidf:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import linear_kernel\n",
    "        \n",
    "        # vectorize/embed movie descriptions\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "        vectors = tfidf_vectorizer.fit_transform(dataset.descriptions)\n",
    "        \n",
    "        # cosine similarity matrix\n",
    "        cossim = linear_kernel(vectors, vectors)\n",
    "        vectors = list(vectors.A)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "        from gensim.test.utils import get_tmpfile\n",
    "        from gensim.similarities import MatrixSimilarity\n",
    "        from gensim.parsing.preprocessing import preprocess_documents\n",
    "        \n",
    "        # extract descriptions from dataset\n",
    "        documents = preprocess_documents(dataset.descriptions.values)\n",
    "        tagged_docs = [TaggedDocument(d, [i]) for i, d in enumerate(documents)]\n",
    "        \n",
    "        # temp file to hasten model\n",
    "        tmpfile = get_tmpfile(\"model\")\n",
    "        \n",
    "        # generate model\n",
    "        try:\n",
    "            model = Doc2Vec(vector_size=vec_size, window=3, min_count=3, workers=8, max_vocab_size=10_000_000)\n",
    "        except:\n",
    "            model = Doc2Vec(vector_size=vec_size, window=2, min_count=3, workers=4, max_vocab_size=10_000_000)\n",
    "        #model.load(\"prj607_model3\")\n",
    "        \n",
    "        # train the model\n",
    "        model.build_vocab(tagged_docs)\n",
    "        model.train(tagged_docs, total_examples=model.corpus_count, epochs=50)\n",
    "        \n",
    "        # delete temp file\n",
    "        model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "        \n",
    "        # save model\n",
    "        modelfile = \"Greene_project607_d2v_model_v1\" \n",
    "        files = os.listdir()\n",
    "        counter = 1\n",
    "        while modelfile in files:\n",
    "            counter += 1\n",
    "            modelfile = modelfile[:29]+str(counter)\n",
    "        model.save(modelfile)\n",
    "        \n",
    "        \n",
    "        # vectorize/embed movie descriptions\n",
    "        vectors = pd.Series([model.infer_vector(documents[x]) for x in range(len(documents))])\n",
    "        \n",
    "        # cosine similarity matrix\n",
    "        cossim = MatrixSimilarity(vectors, num_features=vec_size)        \n",
    "    \n",
    "    #save vector onto dataframe\n",
    "    dataset[\"vector\"] = vectors\n",
    "    \n",
    "    # create movie index\n",
    "    titles = dataset.titles.copy()\n",
    "    titles = titles.apply(lambda x: x.lower())\n",
    "    movie_index = pd.Series(dataset.index, index=titles).drop_duplicates()\n",
    "    \n",
    "    # generate list of duplicate movies\n",
    "    duplicates = dataset.pivot_table(index=\"titles\", aggfunc=\"size\")\n",
    "    duplicates = list(duplicates[duplicates > 1].index.str.lower())\n",
    "    \n",
    "\n",
    "    return dataset, cossim, movie_index, duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_scoring(df, min_voters=1000):\n",
    "    '''\n",
    "    weighted scoring receives a DataFrame and calculates the relative weighted score of each movie\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame,\n",
    "        dataframe containing the movie data to score\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame,\n",
    "        modified dataframe including the weighted scores in a new column \"Weighted_score\"\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    deets = np.array(df[[\"scores\", \"votes\"]])\n",
    "    cluster_score = df.loc[df.votes > min_voters, \"scores\"].mean()\n",
    "    \n",
    "    weighted_list = []\n",
    "    for sample in deets:\n",
    "        avg_score = sample[0]\n",
    "        num_voters = sample[1]\n",
    "        \n",
    "        common_denominator = (num_voters + min_voters)\n",
    "        wr = (num_voters * avg_score) / common_denominator + (cluster_score * min_voters) / common_denominator\n",
    "        weighted_list.append(wr)\n",
    "        \n",
    "    df[\"Weighted_score\"] = weighted_list\n",
    "    \n",
    "    df = df.sort_values(by=\"Weighted_score\", ascending=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_byActor(name, n_movies=5, short=False):\n",
    "    '''\n",
    "    recommend_byActor takes the name of a given actor and returns movies with the actor, ranked amongst\n",
    "    themselves\n",
    "    ****\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str,\n",
    "        name of actor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame,\n",
    "        movie title and other information\n",
    "        \n",
    "    '''\n",
    "    name = name.title()\n",
    "    \n",
    "    # generate dataframe of movie with the actor\n",
    "    df = data.loc[data.stars.str.contains(name)].reset_index()\n",
    "    df.drop(\"index\", axis=1, inplace=True)\n",
    "    \n",
    "    # obtain sorted, weighted scores\n",
    "    df = weighted_scoring(df)\n",
    "    \n",
    "    # drop extraneous columns\n",
    "    df.drop([\"uids\", \"scores\", \"votes\"], axis=1, inplace=True)\n",
    "    \n",
    "    # remove the searched name from the stars list\n",
    "    # the actor is known to be in the returned movies, only return the names of other starring actors\n",
    "    for i in range(df.shape[0]):\n",
    "        tmp = df.loc[i, \"stars\"].replace(\", \",\",\").split(',')\n",
    "        tmp.remove(name)\n",
    "        entry = \", \".join(tmp)\n",
    "        df.loc[i, \"stars\"] = entry\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    df[\"index\"] = list(df.index +1)\n",
    "    \n",
    "    mapper = {\"index\": \"Rank\", \"titles\": \"Film\", \"genres\":\"Genre\", \"ratings\":\"Rating\", \"scores\":\"Score\",\n",
    "              \"lengths\":\"Runtime\", \"directors\":\"Director\", \"stars\": \"Also starring\",\n",
    "              \"descriptions\": \"Description\", \"year\": \"Year\", \"Weighted_score\": \"Score\"}\n",
    "    \n",
    "    # relabel dataframe for output\n",
    "    df = df.rename(columns=mapper)\n",
    "    \n",
    "    if short:\n",
    "        df = df[[\"Rank\", \"Film\", \"Year\", \"Description\"]]\n",
    "    else:\n",
    "        df = df[[\"Rank\",\"Film\",\"Year\",\"Genre\",\"Rating\",\"Runtime\",\"Director\",\"Also starring\",\"Description\",\"Score\"]]\n",
    "\n",
    "    \n",
    "    return df.loc[0:n_movies-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_byTitle(name, cossim, data, n_movies=5, tfidf=False, short=False):\n",
    "    '''\n",
    "    recommend_byTitle takes the name of a movie and returns the top n_movies\n",
    "    with the most similar descriptions from the IMBb database\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : string\n",
    "        title of the movie.\n",
    "    n_movies : int, optional\n",
    "        Number of films to return. The default is 10.\n",
    "    short : bool, optional\n",
    "        switch to return quantity of information. The default is False, returning a dataframe\n",
    "        containing expanded information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        dataframe containing movie information. By default, it returns 10\n",
    "        columns of data\n",
    "\n",
    "    '''\n",
    "\n",
    "    name = name.lower()\n",
    "    \n",
    "    # if the title is unavailable\n",
    "    if name not in movie_index.keys():\n",
    "        return f\"{name.title()} is not in the database.\"\n",
    "\n",
    "    # if there are duplicate names, choose the newest entry\n",
    "    tmp = movie_index[name]\n",
    "    if type(tmp) != type(1):\n",
    "        print(f\"{name.title()} is a duplicate.\")\n",
    "        tmp = tmp[len(tmp)-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "    if tfidf:\n",
    "        idx = tmp\n",
    "    else:\n",
    "        idx = data.iloc[tmp].vector\n",
    "        \n",
    "\n",
    "    # cosine similarity\n",
    "    similarities = sorted(list(enumerate(cossim[idx])), reverse=True, key=lambda x: x[1])\n",
    "    \n",
    "    # list of the first n similar movies by description\n",
    "    simlist = [entry[0] for entry in similarities if entry[0] != tmp]\n",
    "    \n",
    "    # retrieve indices to extract relevant data from primary dataset\n",
    "    indices = simlist[0:n_movies]\n",
    "    \n",
    "    df = data.loc[indices]\n",
    "    \n",
    "    # weighted scoring for internal rankings\n",
    "    df = weighted_scoring(df)\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df[\"index\"] = list(df.index+1)\n",
    "    df.drop([\"uids\",\"scores\", \"votes\"], axis=1, inplace=True)\n",
    "    \n",
    "    # relabel dataframe for output\n",
    "    mapper = {\"index\": \"Rank\", \"titles\": \"Film\", \"genres\":\"Genre\", \"ratings\":\"Rating\",\n",
    "              \"lengths\":\"Runtime\", \"directors\":\"Director\", \"stars\": \"Also starring\",\n",
    "              \"descriptions\": \"Description\", \"year\": \"Year\", \"Weighted_score\": \"Score\"}\n",
    "    \n",
    "    df = df.rename(columns=mapper)\n",
    "    \n",
    "    # truncated dataframe\n",
    "    if short:\n",
    "        df = df[[\"Rank\", \"Film\", \"Year\", \"Description\"]]\n",
    "    else:\n",
    "        df = df[[\"Rank\",\"Film\",\"Year\",\"Genre\",\"Rating\",\"Runtime\",\"Director\",\"Also starring\",\"Description\",\"Score\"]]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
